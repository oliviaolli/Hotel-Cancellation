{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Defines a class for a tree node\n",
    "    '''\n",
    "    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
    "        self.feature = feature #which feature are we splitting on?\n",
    "        self.threshold = threshold #which value of the feature?\n",
    "        self.data_left = data_left #left partition\n",
    "        self.data_right = data_right #right partition\n",
    "        self.gain = gain #infogain\n",
    "        self.value = value #classification of leaf node\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    '''\n",
    "    Implements the DT as a class. Default vals for min_samples_split\n",
    "    and max_depth chosen approximately as\n",
    "    min_samples_split=n^(1/3)\n",
    "    max_depth=features^(1/2)\n",
    "    '''\n",
    "    def __init__(self, min_samples_split=36, max_depth=4,randomized_features=False):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        self.randomized_features=randomized_features\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def _entropy(s):\n",
    "        '''\n",
    "        Helper function to compute entropy based on list of s integer values\n",
    "\n",
    "        Arguments:\n",
    "        s: list\n",
    "\n",
    "        Returns:\n",
    "        entropy: float\n",
    "        '''\n",
    "        counts = np.bincount(np.array(s, dtype=np.int64)) # runtime error w/o np.int64\n",
    "        probs = counts / len(s)  # prob of each label\n",
    "        entropy = -np.sum(probs * np.log2(probs, where=probs > 0))\n",
    "        return entropy\n",
    "\n",
    "    def _information_gain(self, parent, left_child, right_child):\n",
    "        '''\n",
    "        Helper function, computes information_gain\n",
    "\n",
    "        Arguments:\n",
    "        parent: list\n",
    "        left_child: list\n",
    "        right_child: list\n",
    "\n",
    "        Returns:\n",
    "        information_gain: float\n",
    "\n",
    "        '''\n",
    "        left_frac = len(left_child) / len(parent)\n",
    "        right_frac = len(right_child) / len(parent)\n",
    "\n",
    "        information_gain = self._entropy(parent) - (left_frac * self._entropy(left_child) + right_frac * self._entropy(right_child))\n",
    "        return information_gain\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        '''\n",
    "        Helper function, computes the best split given one feature and one target\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, data\n",
    "        y: np.array, targets\n",
    "\n",
    "        Returns:\n",
    "        best_split: dict, info for the node\n",
    "\n",
    "        '''\n",
    "        best_split = {}\n",
    "        best_info_gain = -1\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        if self.randomized_features==True:\n",
    "            tweak_size=int(np.sqrt(n_cols))\n",
    "            chosen_cols=np.random.choice(a=range(n_cols),size=tweak_size,replace=True)\n",
    "        elif self.randomized_features==False:\n",
    "            chosen_cols=range(n_cols)\n",
    "            \n",
    "\n",
    "        for feature in chosen_cols:\n",
    "            X_curr = X[:, feature] #choose splitting feature\n",
    "            for threshold in np.unique(X_curr): #check every unique val in feature\n",
    "\n",
    "                #split X into higher and lower than given unique val \"threshold\"\n",
    "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
    "                df_left = np.array([row for row in df if row[feature] <= threshold])\n",
    "                df_right = np.array([row for row in df if row[feature] > threshold])\n",
    "\n",
    "                \n",
    "                if len(df_left) > 0 and len(df_right) > 0: # If all data is in one df, no need to test\n",
    "                    # extract target vals as last column\n",
    "                    y = df[:, -1]\n",
    "                    y_left = df_left[:, -1]\n",
    "                    y_right = df_right[:, -1]\n",
    "                    \n",
    "                    gain = self._information_gain(y, y_left, y_right)\n",
    "\n",
    "                    if gain > best_info_gain: #save if new split is better than previous best\n",
    "                        best_split = { # save info for node assignment\n",
    "                            'feature_index': feature,\n",
    "                            'threshold': threshold,\n",
    "                            'df_left': df_left,\n",
    "                            'df_right': df_right,\n",
    "                            'gain': gain\n",
    "                        }\n",
    "                        best_info_gain = gain\n",
    "        \n",
    "        return best_split\n",
    "\n",
    "    def _build(self, X, y, depth=0):\n",
    "        '''\n",
    "        Helper recursive function, builds DT\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, data\n",
    "        y: np.array, targets\n",
    "        depth: int, for stopping criteria\n",
    "\n",
    "        Returns:\n",
    "        Node: class instance\n",
    "        '''\n",
    "\n",
    "        n_rows = X.shape[0]\n",
    "        # Get the best split\n",
    "        best = self._best_split(X, y)\n",
    "        \n",
    "        # Check to see if a node should be leaf node\n",
    "        if n_rows >= self.min_samples_split and depth <= self.max_depth and bool(best):\n",
    "            # If it's not a perfect split, build both left and right\n",
    "            if best['gain'] > 0:\n",
    "                # Build a tree on the left\n",
    "                left = self._build(\n",
    "                    X=best['df_left'][:, :-1],\n",
    "                    y=best['df_left'][:, -1],\n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                right = self._build(\n",
    "                    X=best['df_right'][:, :-1],\n",
    "                    y=best['df_right'][:, -1],\n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                return Node(\n",
    "                    feature=best['feature_index'],\n",
    "                    threshold=best['threshold'],\n",
    "                    data_left=left,\n",
    "                    data_right=right,\n",
    "                    gain=best['gain']\n",
    "                )\n",
    "        # Leaf node - value is the most common target value, used for classification\n",
    "        return Node(\n",
    "            value=Counter(y).most_common(1)[0][0]\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Builds DT\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, data\n",
    "        y: np.array, targets\n",
    "        '''\n",
    "        # Call _build starting from the root\n",
    "        self.root = self._build(X, y)\n",
    "\n",
    "    def _predict(self, x, tree):\n",
    "        '''\n",
    "        Helper function, traverses the tree to predict for a single instance\n",
    "\n",
    "        Arguments:\n",
    "        x: np.array, single observation\n",
    "        tree: the trained tree\n",
    "\n",
    "        Returns:\n",
    "        Class prediction: float\n",
    "\n",
    "        '''\n",
    "        # Leaf node\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "\n",
    "        feature_value = x[tree.feature]\n",
    "\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_left)\n",
    "\n",
    "        if feature_value > tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Function calling predict for all instances\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, data\n",
    "\n",
    "        Returns:\n",
    "        Class prediction: np.array\n",
    "        '''\n",
    "        return [self._predict(x, self.root) for x in X] #calls _predict for every instance\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, num_trees=10, min_samples_split=5, max_depth=5,randomized_features=False):\n",
    "        self.num_trees = num_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.randomized_features=randomized_features\n",
    "        # Will store individually trained decision trees\n",
    "        self.decision_trees = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample(X, y):\n",
    "        '''\n",
    "        Samples with replacement (bootstrapping)\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, features\n",
    "        y: np.array, target\n",
    "\n",
    "        Returns:\n",
    "        Sample: tuple (sample of features, sample of target)\n",
    "        '''\n",
    "        n_rows, n_cols = X.shape\n",
    "        # Sample with replacement\n",
    "        samples = np.random.choice(a=n_rows, size=n_rows, replace=True)\n",
    "        Sample=X[samples], y[samples]\n",
    "        return Sample\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the RF classifier by building num_trees decision trees\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, features\n",
    "        y: np.array, target\n",
    "        '''\n",
    "        # Reset\n",
    "        if len(self.decision_trees) > 0:\n",
    "            self.decision_trees = []\n",
    "\n",
    "        # Build each tree of the forest\n",
    "        num_built = 0\n",
    "        while num_built < self.num_trees:\n",
    "            try:\n",
    "                print(\"beginning to build tree nr\", num_built)\n",
    "                clf = DecisionTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    max_depth=self.max_depth,randomized_features=self.randomized_features\n",
    "                )\n",
    "                # Obtain data sample\n",
    "                _X, _y = self._sample(X, y)\n",
    "                # Train\n",
    "                clf.fit(_X, _y)\n",
    "                # Save the classifier\n",
    "                self.decision_trees.append(clf)\n",
    "                print(\"built tree nr\",num_built)\n",
    "                num_built += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts class labels for new data instances.\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, instances to predict\n",
    "\n",
    "        Returns:\n",
    "        Predictions: np.array, predictions for each instance\n",
    "        '''\n",
    "        # Make predictions with every tree in the forest\n",
    "        y = [tree.predict(X) for tree in self.decision_trees]\n",
    "        # Use majority voting for the final prediction\n",
    "        predictions = [max(Counter(preds), key=Counter(preds).get) for preds in np.swapaxes(a=y, axis1=0, axis2=1)]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## DATA LOADING AND CLEANING #############\n",
    "data = pd.read_csv('Hotel Reservations.csv') #load data\n",
    "data_encode = data.copy()\n",
    "#One hot encode\n",
    "labels_to_encode = ['type_of_meal_plan', 'room_type_reserved',\n",
    "                    'market_segment_type']\n",
    "data_encode = pd.get_dummies(data, columns = labels_to_encode) \n",
    "\n",
    "data_encode['booking_status'] = data_encode['booking_status'].astype('category')\n",
    "data_encode['booking_status'] = data_encode['booking_status'].cat.codes\n",
    "data_encode = data_encode[data_encode['no_of_weekend_nights']+data_encode[\"no_of_week_nights\"] > 0]\n",
    "\n",
    "X = np.array(data_encode.drop(['Booking_ID', 'booking_status'], axis=1))\n",
    "y = np.array(data_encode['booking_status'])\n",
    "\n",
    "#sample_size=5000\n",
    "\n",
    "#X=X[:sample_size,:]\n",
    "#y=y[:sample_size]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "\n",
    "data_feature_engineered=data_encode.copy()\n",
    "data_feature_engineered[\"total_nights\"]=data_encode[\"no_of_weekend_nights\"]+data_encode[\"no_of_week_nights\"]\n",
    "data_feature_engineered[\"total_guests\"]=data_encode[\"no_of_adults\"]+data_encode[\"no_of_children\"]\n",
    "data_feature_engineered[\"total_bookings\"]=data_encode[\"no_of_previous_cancellations\"]+data_encode[\"no_of_previous_bookings_not_canceled\"]\n",
    "data_feature_engineered[\"total_cost\"]=data_feature_engineered[\"total_nights\"]*data_feature_engineered[\"avg_price_per_room\"]\n",
    "\n",
    "X_feature_engineered = np.array(data_feature_engineered.drop(['Booking_ID', 'booking_status'], axis=1))\n",
    "y_feature_engineered = np.array(data_feature_engineered['booking_status'])\n",
    "\n",
    "#X_feature_engineered=X_feature_engineered[:sample_size,:]\n",
    "#y_feature_engineered=y_feature_engineered[:sample_size]\n",
    "\n",
    "X_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(X_feature_engineered, y_feature_engineered, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ PCA ############\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_s=scaler.transform(X_train)\n",
    "X_test_s=scaler.transform(X_test)\n",
    "\n",
    "pca=PCA(0.95)\n",
    "pca.fit(X_train_s)\n",
    "X_pca_train = pca.transform(X_train_s)\n",
    "X_pca_test = pca.transform(X_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TRAINING MODEL ON OUR IMPLEMENTATION ######\n",
    "model_dt = DecisionTree()\n",
    "model_dt.fit(X_train, y_train)\n",
    "\n",
    "preds_train = model_dt.predict(X_train)\n",
    "preds_test = model_dt.predict(X_test)\n",
    "\n",
    "print(\"DT train accuracy:\",accuracy_score(y_train, preds_train))\n",
    "print(\"DT test accuracy:\",accuracy_score(y_test, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ BASE RANDOM FOREST ######\n",
    "start = timeit.default_timer()\n",
    "print(\"Training base random forest\")\n",
    "model_rf = RandomForest(num_trees=16,min_samples_split=4,max_depth=32)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "preds_train = model_rf.predict(X_train)\n",
    "preds_test = model_rf.predict(X_test)\n",
    "print(\"Base random forrest train accuracy:\",accuracy_score(y_train, preds_train))\n",
    "print(\"Base random forrest test accuracy:\",accuracy_score(y_test, preds_test))\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ BASE RANDOM FOREST WITH PCA######\n",
    "print(\"Training base random forest\")\n",
    "start = timeit.default_timer()\n",
    "model_rf = RandomForest(num_trees=16,min_samples_split=4,max_depth=32)\n",
    "model_rf.fit(X_pca_train, y_train)\n",
    "\n",
    "preds_train = model_rf.predict(X_pca_train)\n",
    "preds_test = model_rf.predict(X_pca_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Base random forrest train accuracy:\",accuracy_score(y_train, preds_train))\n",
    "print(\"Base random forrest test accuracy:\",accuracy_score(y_test, preds_test))\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base random forrest for feature engineered data set\n",
      "beginning to build tree nr 0\n",
      "built tree nr 0\n",
      "beginning to build tree nr 1\n",
      "built tree nr 1\n",
      "beginning to build tree nr 2\n",
      "built tree nr 2\n",
      "beginning to build tree nr 3\n",
      "built tree nr 3\n",
      "beginning to build tree nr 4\n",
      "built tree nr 4\n",
      "beginning to build tree nr 5\n",
      "built tree nr 5\n",
      "beginning to build tree nr 6\n",
      "built tree nr 6\n",
      "beginning to build tree nr 7\n",
      "built tree nr 7\n",
      "beginning to build tree nr 8\n",
      "built tree nr 8\n",
      "beginning to build tree nr 9\n",
      "built tree nr 9\n",
      "beginning to build tree nr 10\n",
      "built tree nr 10\n",
      "beginning to build tree nr 11\n",
      "built tree nr 11\n",
      "beginning to build tree nr 12\n",
      "built tree nr 12\n",
      "beginning to build tree nr 13\n"
     ]
    }
   ],
   "source": [
    "############ FEATURE ENGINEREERED RANDOM FOREST ######\n",
    "start = timeit.default_timer()\n",
    "print(\"Training base random forrest for feature engineered data set\")\n",
    "model_fe = RandomForest(num_trees=16,min_samples_split=4,max_depth=32)\n",
    "model_fe.fit(X_train_fe, y_train_fe)\n",
    "\n",
    "preds_train_fe = model_fe.predict(X_train_fe)\n",
    "preds_test_fe = model_fe.predict(X_test_fe)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(\"FE data train accuracy:\",accuracy_score(y_train_fe, preds_train_fe))\n",
    "print(\"FE data test accuracy:\",accuracy_score(y_test_fe, preds_test_fe))\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ FEATURE ENGINEREERED RANDOM FOREST WITH RANDOMIZATION ######\n",
    "print(\"Training randomized forrest for feature engineered data set\")\n",
    "start = timeit.default_timer()\n",
    "model_fe_rand = RandomForest(num_trees=16,min_samples_split=4,max_depth=32,randomized_features=True)\n",
    "model_fe_rand.fit(X_train_fe, y_train_fe)\n",
    "\n",
    "preds_train_rand = model_fe_rand.predict(X_train_fe)\n",
    "preds_test_rand = model_fe_rand.predict(X_test_fe)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"FE data train accuracy:\",accuracy_score(y_train_fe, preds_train_rand))\n",
    "print(\"FE data test accuracy:\",accuracy_score(y_test_fe, preds_test_rand))\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA random forest, timetest\n",
    "X=X[0:500,]\n",
    "y=y[0:500]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "#Do PCA\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train_s=scaler.transform(X_train)\n",
    "X_test_s=scaler.transform(X_test)\n",
    "pca=PCA(0.95)\n",
    "pca.fit(X_train_s)\n",
    "X_pca_train = pca.transform(X_train_s)\n",
    "X_pca_test = pca.transform(X_test_s)\n",
    "\n",
    "\n",
    "#Random forest with PCA\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "max_features=None\n",
    "rf = RandomForest(num_trees=16,max_depth=32,min_samples_split=4)\n",
    "rf.fit(X_pca_train, y_train)\n",
    "rf_train_accuracy = accuracy_score(y_train, rf.predict(X_pca_train))\n",
    "rf_test_accuracy = accuracy_score(y_test, rf.predict(X_pca_test))\n",
    "print(\"train\", rf_train_accuracy, \"test\",rf_test_accuracy)\n",
    "stop = timeit.default_timer()\n",
    "print('Time with PCA: ', stop - start) \n",
    "\n",
    "#Random forest without\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "max_features=None\n",
    "rf = RandomForest(num_trees=16,max_depth=32,min_samples_split=4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_train_accuracy = accuracy_score(y_train, rf.predict(X_train))\n",
    "rf_test_accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "print(\"train\", rf_train_accuracy, \"test\",rf_test_accuracy)\n",
    "stop = timeit.default_timer()\n",
    "print('Time without PCA: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
