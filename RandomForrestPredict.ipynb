{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class Knude:\n",
    "    '''\n",
    "    Defines a class for a tree node (Danish: Knude)\n",
    "    '''\n",
    "    def __init__(self, feature=None, threshold=None, inf_gain=None, classification=None, left=None, right=None):\n",
    "        self.feature = feature #which feature are we splitting on?\n",
    "        self.threshold = threshold #which value of the feature?\n",
    "        self.inf_gain = inf_gain #infogain\n",
    "        self.classification = classification #classification of leaf node\n",
    "        self.left = left #left partition\n",
    "        self.right = right #right partition\n",
    "\n",
    "class DecisionTree:\n",
    "    '''\n",
    "    Implements the DT as a class. Default vals for min_leaf_size\n",
    "    and max_height chosen approximately as\n",
    "    min_leaf_size=n^(1/3)\n",
    "    max_height=features^(1/2)\n",
    "    '''\n",
    "    def __init__(self, min_leaf_size=36, max_height=4,randomized_features=False):\n",
    "        self.root = None\n",
    "        self.randomized_features=randomized_features\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        self.max_height = max_height\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_entropy(p):\n",
    "        '''\n",
    "        Helper function to compute entropy based on list of p integer values\n",
    "\n",
    "        Arguments:\n",
    "        p: list\n",
    "\n",
    "        Returns:\n",
    "        entropy: float\n",
    "        '''\n",
    "        count = np.bincount(np.array(p, dtype=np.int64)) # runtime error w/o np.int64\n",
    "        probs = count / len(p)  # prob of each label\n",
    "        entropy = -np.sum(probs * np.log2(probs, where=probs > 0))\n",
    "        return entropy\n",
    "\n",
    "    def _compute_information_gain(self, parent, left_branch, right_branch):\n",
    "        '''\n",
    "        Helper function, computes information_gain\n",
    "\n",
    "        Arguments:\n",
    "        parent: list\n",
    "        left_branch: list\n",
    "        right_branch: list\n",
    "\n",
    "        Returns:\n",
    "        information_gain: float\n",
    "\n",
    "        '''\n",
    "        left_frac = len(left_branch) / len(parent)\n",
    "        right_frac = len(right_branch) / len(parent)\n",
    "\n",
    "        information_gain = self._compute_entropy(parent) - (left_frac * self._compute_entropy(left_branch) + right_frac * self._compute_entropy(right_branch))\n",
    "        return information_gain\n",
    "\n",
    "    def _compute_best_split(self, X, y):\n",
    "        '''\n",
    "        Helper function, takes X and y as inputs and finds the feature and values to split on to \n",
    "        get the biggest information gain. If randomized_features==True, it will only search\n",
    "        through a randomized subset of the columns, otherwise it will search through \n",
    "        all columns\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, data\n",
    "        y: np.array, targets\n",
    "\n",
    "        Returns:\n",
    "        best_split: dict, info for the node\n",
    "\n",
    "        '''\n",
    "        \n",
    "        n_cols = X.shape[1]\n",
    "        best_split = {}\n",
    "        best_info_gain = -2\n",
    "        \n",
    "        if self.randomized_features==True:\n",
    "            tweak_size=int(np.sqrt(n_cols))\n",
    "            chosen_cols=np.random.choice(a=range(n_cols),size=tweak_size,replace=True)\n",
    "        elif self.randomized_features==False:\n",
    "            chosen_cols=range(n_cols)\n",
    "            \n",
    "\n",
    "        for feature in chosen_cols:\n",
    "            X_curr = X[:, feature] #choose splitting feature\n",
    "            split_vals=np.unique(X_curr)\n",
    "            for split_val in split_vals: #check every unique val in feature\n",
    "\n",
    "                #split X into higher and lower than given unique val \"threshold\"\n",
    "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
    "                left = np.array([r for r in df if r[feature] <= split_val])\n",
    "                right = np.array([r for r in df if r[feature] > split_val])\n",
    "\n",
    "                \n",
    "                if len(left) > 0 and len(right) > 0: # If all data is in one df, no need to test\n",
    "                    # extract target vals as last column\n",
    "                    y = df[:, -1]\n",
    "                    y_left = left[:, -1]\n",
    "                    y_right = right[:, -1]\n",
    "                    \n",
    "                    inf_gain = self._compute_information_gain(y, y_left, y_right)\n",
    "\n",
    "                    if inf_gain > best_info_gain: #save if new split is better than previous best\n",
    "                        best_split = { # save info for node assignment\n",
    "                            'feature_index': feature,\n",
    "                            'threshold': split_val,\n",
    "                            'df_left': left,\n",
    "                            'df_right': right,\n",
    "                            'inf_gain': inf_gain\n",
    "                        }\n",
    "                        best_info_gain = inf_gain\n",
    "        \n",
    "        return best_split\n",
    "\n",
    "    def _build_tree(self, X, y, height=0):\n",
    "        '''\n",
    "        Helper function, builds DT for one point\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, data\n",
    "        y: np.array, targets\n",
    "        height: int, for stopping criteria\n",
    "\n",
    "        Returns:\n",
    "        node: class instance\n",
    "        '''\n",
    "\n",
    "        n_rows = X.shape[0]\n",
    "        # Get the best split\n",
    "        best = self._compute_best_split(X, y)\n",
    "        \n",
    "        # Check to see if a node should be leaf node\n",
    "        if n_rows >= self.min_leaf_size and height <= self.max_height and bool(best):\n",
    "            if best['inf_gain'] > 0:\n",
    "                # Build a tree on the left\n",
    "                left = self._build_tree(\n",
    "                    X=best['df_left'][:, :-1],\n",
    "                    y=best['df_left'][:, -1],\n",
    "                    height=height + 1\n",
    "                )\n",
    "                right = self._build_tree(\n",
    "                    X=best['df_right'][:, :-1],\n",
    "                    y=best['df_right'][:, -1],\n",
    "                    height=height + 1\n",
    "                )\n",
    "                return Knude(\n",
    "                    feature=best['feature_index'],\n",
    "                    threshold=best['threshold'],\n",
    "                    left=left,\n",
    "                    right=right,\n",
    "                    inf_gain=best['inf_gain']\n",
    "                )\n",
    "        # Leaf node\n",
    "        return Knude(\n",
    "            classification=Counter(y).most_common(1)[0][0]\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Builds DT\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, features\n",
    "        y: np.array, targets\n",
    "        '''\n",
    "        # Call _build_tree starting from the root\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _predict_instance(self, x, tree):\n",
    "        '''\n",
    "        Helper function, predicts classification recursevely for one point\n",
    "\n",
    "        Arguments:\n",
    "        x: np.array, single observation\n",
    "        tree: the trained tree\n",
    "\n",
    "        Returns:\n",
    "        Class prediction: float\n",
    "\n",
    "        '''\n",
    "        # Checks if its a node\n",
    "        if tree.classification != None:\n",
    "            return tree.classification\n",
    "\n",
    "        X_feature = x[tree.feature] #extracts the feature to be split on\n",
    "\n",
    "        if X_feature <= tree.threshold:\n",
    "            return self._predict_instance(x=x, tree=tree.left) #get prediction from left part\n",
    "\n",
    "        if X_feature > tree.threshold:\n",
    "            return self._predict_instance(x=x, tree=tree.right) #get prediction from right part\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Function calling predict for all instances\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, data\n",
    "\n",
    "        Returns:\n",
    "        Class prediction: np.array\n",
    "        '''\n",
    "        return [self._predict_instance(x, self.root) for x in X] #calls _predict_instance for every instance\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, randomized_features=False,n_trees=10, max_height=5,min_leaf_size=5):\n",
    "        self.randomized_features=randomized_features\n",
    "        self.n_trees = n_trees\n",
    "        self.max_height = max_height\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        # Will store individually trained decision trees\n",
    "        self.tree_list = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _bootstrap(X, y):\n",
    "        '''\n",
    "        Bootstraps based on X and y\n",
    "        '''\n",
    "        # Sample with replacement\n",
    "        samples = np.random.choice(a=X.shape[0], size=X.shape[0], replace=True)\n",
    "        feature_sample=X[samples]\n",
    "        target_sample=y[samples]\n",
    "        return feature_sample,target_sample\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the RF classifier by building n_trees decision trees\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, features\n",
    "        y: np.array, target\n",
    "        '''\n",
    "        # Reset\n",
    "        if len(self.tree_list) > 0:\n",
    "            self.tree_list = []\n",
    "\n",
    "        # Build each tree of the forest\n",
    "        trees_built = 0\n",
    "        while trees_built < self.n_trees:\n",
    "            try:\n",
    "                print(\"beginning to build tree nr\", trees_built)\n",
    "                dt = DecisionTree(\n",
    "                    min_leaf_size = self.min_leaf_size,\n",
    "                    max_height = self.max_height,randomized_features = self.randomized_features\n",
    "                )\n",
    "                # Obtain data sample\n",
    "                X_bootstrapped, y_bootstrapped = self._bootstrap(X, y)\n",
    "                # Train\n",
    "                dt.fit(X_bootstrapped, y_bootstrapped)\n",
    "                # Save the classifier\n",
    "                self.tree_list.append(dt)\n",
    "                print(\"built tree nr\",trees_built)\n",
    "                trees_built = trees_built+1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts labels for all test instances\n",
    "\n",
    "        Arguments:\n",
    "        X: np.array, instances to predict\n",
    "\n",
    "        Returns:\n",
    "        Predictions: np.array, predictions for each instance\n",
    "        '''\n",
    "        # Make predictions with every tree in the forest\n",
    "        predictions = [tree.predict(X) for tree in self.tree_list]\n",
    "        # Use majority voting for the final prediction\n",
    "        prediction = [max(Counter(preds), key=Counter(preds).get) for preds in np.swapaxes(a=predictions, axis1=0, axis2=1)]\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## DATA LOADING AND CLEANING #############\n",
    "data = pd.read_csv('Hotel Reservations.csv') #load data\n",
    "data_encode = data.copy()\n",
    "#One hot encode\n",
    "labels_to_encode = ['type_of_meal_plan', 'room_type_reserved',\n",
    "                    'market_segment_type']\n",
    "data_encode = pd.get_dummies(data, columns = labels_to_encode) \n",
    "\n",
    "data_encode['booking_status'] = data_encode['booking_status'].astype('category')\n",
    "data_encode['booking_status'] = data_encode['booking_status'].cat.codes\n",
    "data_encode = data_encode[data_encode['no_of_weekend_nights']+data_encode[\"no_of_week_nights\"] > 0]\n",
    "\n",
    "X = np.array(data_encode.drop(['Booking_ID', 'booking_status'], axis=1))\n",
    "y = np.array(data_encode['booking_status'])\n",
    "\n",
    "#sample_size=5000\n",
    "\n",
    "#X=X[:sample_size,:]\n",
    "#y=y[:sample_size]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "\n",
    "data_feature_engineered=data_encode.copy()\n",
    "data_feature_engineered[\"total_nights\"]=data_encode[\"no_of_weekend_nights\"]+data_encode[\"no_of_week_nights\"]\n",
    "data_feature_engineered[\"total_guests\"]=data_encode[\"no_of_adults\"]+data_encode[\"no_of_children\"]\n",
    "data_feature_engineered[\"total_bookings\"]=data_encode[\"no_of_previous_cancellations\"]+data_encode[\"no_of_previous_bookings_not_canceled\"]\n",
    "data_feature_engineered[\"total_cost\"]=data_feature_engineered[\"total_nights\"]*data_feature_engineered[\"avg_price_per_room\"]\n",
    "\n",
    "X_feature_engineered = np.array(data_feature_engineered.drop(['Booking_ID', 'booking_status'], axis=1))\n",
    "y_feature_engineered = np.array(data_feature_engineered['booking_status'])\n",
    "\n",
    "#X_feature_engineered=X_feature_engineered[:sample_size,:]\n",
    "#y_feature_engineered=y_feature_engineered[:sample_size]\n",
    "\n",
    "X_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(X_feature_engineered, y_feature_engineered, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ PCA ############\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_s=scaler.transform(X_train)\n",
    "X_test_s=scaler.transform(X_test)\n",
    "\n",
    "pca=PCA(0.95)\n",
    "pca.fit(X_train_s)\n",
    "X_pca_train = pca.transform(X_train_s)\n",
    "X_pca_test = pca.transform(X_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TRAINING MODEL ON OUR IMPLEMENTATION ######\n",
    "model_dt = DecisionTree()\n",
    "model_dt.fit(X_train, y_train)\n",
    "\n",
    "preds_train = model_dt.predict(X_train)\n",
    "preds_test = model_dt.predict(X_test)\n",
    "\n",
    "print(\"DT train accuracy:\",accuracy_score(y_train, preds_train))\n",
    "print(\"DT test accuracy:\",accuracy_score(y_test, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ BASE RANDOM FOREST ######\n",
    "start = timeit.default_timer()\n",
    "print(\"Training base random forest\")\n",
    "model_rf = RandomForest(n_trees=16,min_leaf_size=4,max_height=32)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "preds_train = model_rf.predict(X_train)\n",
    "preds_test = model_rf.predict(X_test)\n",
    "print(\"Base random forrest train accuracy:\",accuracy_score(y_train, preds_train))\n",
    "print(\"Base random forrest test accuracy:\",accuracy_score(y_test, preds_test))\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ BASE RANDOM FOREST WITH PCA######\n",
    "print(\"Training base random forest\")\n",
    "start = timeit.default_timer()\n",
    "model_rf = RandomForest(n_trees=16,min_leaf_size=4,max_height=32)\n",
    "model_rf.fit(X_pca_train, y_train)\n",
    "\n",
    "preds_train = model_rf.predict(X_pca_train)\n",
    "preds_test = model_rf.predict(X_pca_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Base random forrest train accuracy:\",accuracy_score(y_train, preds_train))\n",
    "print(\"Base random forrest test accuracy:\",accuracy_score(y_test, preds_test))\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base random forrest for feature engineered data set\n",
      "beginning to build tree nr 0\n",
      "built tree nr 0\n",
      "beginning to build tree nr 1\n",
      "built tree nr 1\n",
      "beginning to build tree nr 2\n",
      "built tree nr 2\n",
      "beginning to build tree nr 3\n",
      "built tree nr 3\n",
      "beginning to build tree nr 4\n",
      "built tree nr 4\n",
      "beginning to build tree nr 5\n",
      "built tree nr 5\n",
      "beginning to build tree nr 6\n",
      "built tree nr 6\n",
      "beginning to build tree nr 7\n",
      "built tree nr 7\n",
      "beginning to build tree nr 8\n",
      "built tree nr 8\n",
      "beginning to build tree nr 9\n",
      "built tree nr 9\n",
      "beginning to build tree nr 10\n",
      "built tree nr 10\n",
      "beginning to build tree nr 11\n",
      "built tree nr 11\n",
      "beginning to build tree nr 12\n",
      "built tree nr 12\n",
      "beginning to build tree nr 13\n",
      "built tree nr 13\n",
      "beginning to build tree nr 14\n",
      "built tree nr 14\n",
      "beginning to build tree nr 15\n",
      "built tree nr 15\n",
      "FE data train accuracy: 0.9887764616500329\n",
      "FE data test accuracy: 0.8940607734806629\n",
      "Time:  10794.548590400002\n"
     ]
    }
   ],
   "source": [
    "############ FEATURE ENGINEREERED RANDOM FOREST ######\n",
    "start = timeit.default_timer()\n",
    "print(\"Training base random forrest for feature engineered data set\")\n",
    "model_fe = RandomForest(n_trees=16,min_leaf_size=4,max_height=32)\n",
    "model_fe.fit(X_train_fe, y_train_fe)\n",
    "\n",
    "preds_train_fe = model_fe.predict(X_train_fe)\n",
    "preds_test_fe = model_fe.predict(X_test_fe)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(\"FE data train accuracy:\",accuracy_score(y_train_fe, preds_train_fe))\n",
    "print(\"FE data test accuracy:\",accuracy_score(y_test_fe, preds_test_fe))\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training randomized forrest for feature engineered data set\n",
      "beginning to build tree nr 0\n"
     ]
    }
   ],
   "source": [
    "############ FEATURE ENGINEREERED RANDOM FOREST WITH RANDOMIZATION ######\n",
    "print(\"Training randomized forrest for feature engineered data set\")\n",
    "start = timeit.default_timer()\n",
    "model_rand = RandomForest(n_trees=16,min_leaf_size=4,max_height=32,randomized_features=True)\n",
    "model_rand.fit(X_train, y_train)\n",
    "\n",
    "preds_train_rand = model_rand.predict(X_train)\n",
    "preds_test_rand = model_rand.predict(X_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Random data train accuracy:\",accuracy_score(y_train, preds_train_rand))\n",
    "print(\"Random data test accuracy:\",accuracy_score(y_test, preds_test_rand))\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training randomized forrest for feature engineered data set\n",
      "beginning to build tree nr 0\n",
      "built tree nr 0\n",
      "beginning to build tree nr 1\n",
      "built tree nr 1\n",
      "beginning to build tree nr 2\n",
      "built tree nr 2\n",
      "beginning to build tree nr 3\n",
      "built tree nr 3\n",
      "beginning to build tree nr 4\n",
      "built tree nr 4\n",
      "beginning to build tree nr 5\n",
      "built tree nr 5\n",
      "beginning to build tree nr 6\n",
      "built tree nr 6\n",
      "beginning to build tree nr 7\n",
      "built tree nr 7\n",
      "beginning to build tree nr 8\n",
      "built tree nr 8\n",
      "beginning to build tree nr 9\n",
      "built tree nr 9\n",
      "beginning to build tree nr 10\n",
      "built tree nr 10\n",
      "beginning to build tree nr 11\n",
      "built tree nr 11\n",
      "beginning to build tree nr 12\n",
      "built tree nr 12\n",
      "beginning to build tree nr 13\n",
      "built tree nr 13\n",
      "beginning to build tree nr 14\n",
      "built tree nr 14\n",
      "beginning to build tree nr 15\n",
      "built tree nr 15\n",
      "FE data train accuracy: 0.9256483751769866\n",
      "FE data test accuracy: 0.8863259668508288\n",
      "Time:  2143.4467369000013\n"
     ]
    }
   ],
   "source": [
    "############ FEATURE ENGINEREERED RANDOM FOREST WITH RANDOMIZATION ######\n",
    "print(\"Training randomized forrest for feature engineered data set\")\n",
    "start = timeit.default_timer()\n",
    "model_fe_rand = RandomForest(n_trees=16,min_leaf_size=4,max_height=32,randomized_features=True)\n",
    "model_fe_rand.fit(X_train_fe, y_train_fe)\n",
    "\n",
    "preds_train_rand = model_fe_rand.predict(X_train_fe)\n",
    "preds_test_rand = model_fe_rand.predict(X_test_fe)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"FE data train accuracy:\",accuracy_score(y_train_fe, preds_train_rand))\n",
    "print(\"FE data test accuracy:\",accuracy_score(y_test_fe, preds_test_rand))\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA random forest, timetest\n",
    "X=X[0:500,]\n",
    "y=y[0:500]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "#Do PCA\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train_s=scaler.transform(X_train)\n",
    "X_test_s=scaler.transform(X_test)\n",
    "pca=PCA(0.95)\n",
    "pca.fit(X_train_s)\n",
    "X_pca_train = pca.transform(X_train_s)\n",
    "X_pca_test = pca.transform(X_test_s)\n",
    "\n",
    "\n",
    "#Random forest with PCA\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "max_features=None\n",
    "rf = RandomForest(n_trees=16,max_height=32,min_leaf_size=4)\n",
    "rf.fit(X_pca_train, y_train)\n",
    "rf_train_accuracy = accuracy_score(y_train, rf.predict(X_pca_train))\n",
    "rf_test_accuracy = accuracy_score(y_test, rf.predict(X_pca_test))\n",
    "print(\"train\", rf_train_accuracy, \"test\",rf_test_accuracy)\n",
    "stop = timeit.default_timer()\n",
    "print('Time with PCA: ', stop - start) \n",
    "\n",
    "#Random forest without\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "max_features=None\n",
    "rf = RandomForest(n_trees=16,max_height=32,min_leaf_size=4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_train_accuracy = accuracy_score(y_train, rf.predict(X_train))\n",
    "rf_test_accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "print(\"train\", rf_train_accuracy, \"test\",rf_test_accuracy)\n",
    "stop = timeit.default_timer()\n",
    "print('Time without PCA: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
